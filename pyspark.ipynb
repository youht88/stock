{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T09:32:03.582261Z",
     "start_time": "2019-06-05T09:32:02.527660Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "dir(SparkSession)\n",
    "spark = SparkSession\\\n",
    "    .builder.master('local')\\\n",
    "    .config(\"spark.default.parallelism\",2)\\\n",
    "    .appName('test')\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc=spark.sparkContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tushare as ts\n",
    "a=np.arange(100000).reshape(1000,100)\n",
    "df=spark.createDataFrame(pd.DataFrame(a))\n",
    "rdd=sc.parallelize(range(10),2)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T09:51:00.068260Z",
     "start_time": "2019-06-05T09:50:59.931974Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize([1,2,3,4,5])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:07:01.198008Z",
     "start_time": "2019-06-05T07:07:01.185825Z"
    }
   },
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName('learn_ml').master('local[1]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T08:36:21.668756Z",
     "start_time": "2019-06-04T08:36:21.332175Z"
    }
   },
   "outputs": [],
   "source": [
    "#ts1=ts.get_today_ticks('300096')\n",
    "ts1.head()\n",
    "df = spark.createDataFrame(ts1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T09:19:29.013811Z",
     "start_time": "2019-06-04T09:19:24.518775Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.crosstab(\"price\",\"type\").sort(\"price_type\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T15:13:50.015106Z",
     "start_time": "2019-06-05T15:13:50.002525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['两块', '牛肉', '洋葱', '鸡蛋']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "list1=jieba.lcut(\"两块牛肉、洋葱、鸡蛋\",HMM=True)\n",
    "list2=list(filter(lambda x: re.match('\\w', x) != None, list1))\n",
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T15:14:10.383629Z",
     "start_time": "2019-06-05T15:14:10.376385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `jieba.cut()` not found.\n"
     ]
    }
   ],
   "source": [
    "jieba.cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:17:15.685519Z",
     "start_time": "2019-06-05T12:17:05.517154Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName('learn_ml').master('local[1]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:01.810485Z",
     "start_time": "2019-06-05T12:51:01.767235Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(0.5,),(1.0,),(1.5,),(2.0,)], ['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:02.204818Z",
     "start_time": "2019-06-05T12:51:01.813610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|values|features|\n",
      "+------+--------+\n",
      "|   0.5|     0.0|\n",
      "|   1.0|     1.0|\n",
      "|   1.5|     1.0|\n",
      "|   2.0|     1.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=0.7, inputCol=\"values\", outputCol=\"features\")\n",
    "binarizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:02.405823Z",
     "start_time": "2019-06-05T12:51:02.210863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|values|freqs|\n",
      "+------+-----+\n",
      "|   0.5|  0.0|\n",
      "|   1.0|  1.0|\n",
      "|   1.5|  1.0|\n",
      "|   2.0|  1.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过setParams，更改配置\n",
    "binarizer.setParams(outputCol=\"freqs\").transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:02.726554Z",
     "start_time": "2019-06-05T12:51:02.410174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|values|vector|\n",
      "+------+------+\n",
      "|   0.5|   0.0|\n",
      "|   1.0|   0.0|\n",
      "|   1.5|   1.0|\n",
      "|   2.0|   1.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过params更改配置\n",
    "params = {binarizer.threshold: 1.2, binarizer.outputCol: \"vector\"}\n",
    "binarizer.transform(df, params).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:03.044037Z",
     "start_time": "2019-06-05T12:51:02.729993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|values|buckets|\n",
      "+------+-------+\n",
      "|   0.1|    0.0|\n",
      "|   0.4|    0.0|\n",
      "|   1.2|    1.0|\n",
      "|   1.5|    2.0|\n",
      "|   NaN|    3.0|\n",
      "|   NaN|    3.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark.ml.feature.Bucketizer(self, splits=None, inputCol=None, outputCol=None, handleInvalid=\"error\")\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, [\"values\"])\n",
    "# splits 为分类区间\n",
    "bucketizer = Bucketizer(splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],inputCol=\"values\", outputCol=\"buckets\")\n",
    "# 这里setHandleInvalid是对nan值进行处理，默认是error：有nan则报错；keep：将nan保留为新分类；skip：忽略nan值\n",
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:05.532429Z",
     "start_time": "2019-06-05T12:51:03.047520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------------+\n",
      "|          features|label|selectedFeatures|\n",
      "+------------------+-----+----------------+\n",
      "|[0.0,0.0,18.0,1.0]|  1.0|      [18.0,1.0]|\n",
      "|[0.0,1.0,12.0,0.0]|  0.0|      [12.0,0.0]|\n",
      "|[1.0,0.0,15.0,0.1]|  0.0|      [15.0,0.1]|\n",
      "+------------------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''yspark.ml.feature.ChiSqSelector(self, numTopFeatures=50, featuresCol=\"features\", outputCol=None, labelCol=\"label\", selectorType=\"numTopFeatures\", percentile=0.1, fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "对于分类目标变量（思考分类模型），此功能允许你选择预定义数量的特征（由numTopFeatures参数进行参数化），以便最好地说明目标的变化。该方法需要两部：需要.fit()——可以计算卡方检验，调用.fit()方法，将DataFrame作为参数传入返回一个ChiSqSelectorModel对象，然后可以使用该对象的.transform()方法来转换DataFrame。默认情况下，选择方法是numTopFeatures，默认顶级要素数设置为50。 percentile 相识于num ，选取百分比的特征 fpr 选择p-values低于阈值的所有特征，从而控制误差的选择率。 fdr 使用 Benjamini-Hochberg procedure fwe 选择p-values低于阈值的所有特征。阈值按1 / numFeatures缩放\n",
    "'''\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "df = spark.createDataFrame(\n",
    "[(Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0),\n",
    "(Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0),\n",
    "(Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0)],\n",
    "[\"features\", \"label\"])\n",
    "selector = ChiSqSelector(numTopFeatures=2, outputCol=\"selectedFeatures\")\n",
    "model = selector.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:06.474248Z",
     "start_time": "2019-06-05T12:51:05.535914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------------+\n",
      "|label|raw            |vectors                  |\n",
      "+-----+---------------+-------------------------+\n",
      "|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+-----+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''pyspark.ml.feature.CountVectorizer(self, minTF=1.0, minDF=1.0, vocabSize=1 << 18, binary=False, inputCol=None, outputCol=None)¶\n",
    "从文档集合中提取词汇表并生成向量'''\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "df = spark.createDataFrame(\n",
    "[(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "[\"label\", \"raw\"])\n",
    "cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "model = cv.fit(df)\n",
    "model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:06.824750Z",
     "start_time": "2019-06-05T12:51:06.478161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|       values|        eprod|\n",
      "+-------------+-------------+\n",
      "|[2.0,1.0,3.0]|[2.0,2.0,9.0]|\n",
      "+-------------+-------------+\n",
      "\n",
      "+-------------+--------------+\n",
      "|       values|         eprod|\n",
      "+-------------+--------------+\n",
      "|[2.0,1.0,3.0]|[4.0,3.0,15.0]|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark.ml.feature.ElementwiseProduct(scalingVec=None, inputCol=None, outputCol=None)\n",
    "#使用提供的“权重”向量输出每个输入向量的阿达马乘积（即，逐元素乘积）。换句话说，它通过标量乘数缩放数据集的每一列。\n",
    "from pyspark.ml.feature import ElementwiseProduct \n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([2.0, 1.0, 3.0]),)], [\"values\"])\n",
    "ep = ElementwiseProduct(scalingVec=Vectors.dense([1.0, 2.0, 3.0]),\n",
    "inputCol=\"values\", outputCol=\"eprod\")\n",
    "ep.transform(df).show()\n",
    "ep.setParams(scalingVec=Vectors.dense([2.0, 3.0, 5.0])).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:08.184303Z",
     "start_time": "2019-06-05T12:51:06.832059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|NaN|\n",
      "|NaN|3.0|\n",
      "|4.0|4.0|\n",
      "|5.0|5.0|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|3.0|4.0|\n",
      "+---+---+\n",
      "\n",
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pyspark.ml.feature.Imputer(*args, **kwargs)¶\n",
    "用于完成缺失值的插补估计器，使用缺失值所在列的平均值或中值。 输入列应该是DoubleType或FloatType。 目前的Imputer不支持分类特征，可能会为分类特征创建不正确的值。 请注意，平均值/中值是在过滤出缺失值之后计算的。 输入列中的所有Null值都被视为缺失，所以也被归类。 为了计算中位数，使用pyspark.sql.DataFrame.approxQuantile（），相对误差为0.001。\n",
    "'''\n",
    "from pyspark.ml.feature import Imputer\n",
    "df = spark.createDataFrame([(1.0, float(\"nan\")), (2.0, float(\"nan\")), (float(\"nan\"), 3.0),\n",
    "                             (4.0, 4.0), (5.0, 5.0)], [\"a\", \"b\"])\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "df.show()\n",
    "model.surrogateDF.show()\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:08.751087Z",
     "start_time": "2019-06-05T12:51:08.192710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  2.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.setStrategy(\"median\").setMissingValue(float(\"nan\")).fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:09.220031Z",
     "start_time": "2019-06-05T12:51:08.754945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|    a|scaled|\n",
      "+-----+------+\n",
      "|[1.0]| [0.5]|\n",
      "|[2.0]| [1.0]|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark.ml.feature.MaxAbsScaler(self, inputCol=None, outputCol=None)¶\n",
    "#通过分割每个特征中的最大绝对值来单独重新缩放每个特征以范围[-1,1]。 它不会移动/居中数据，因此不会破坏任何稀疏性\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([1.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "maScaler = MaxAbsScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = maScaler.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:09.596547Z",
     "start_time": "2019-06-05T12:51:09.222882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0] [2.0]\n",
      "+-----+------+\n",
      "|    a|scaled|\n",
      "+-----+------+\n",
      "|[0.0]| [0.0]|\n",
      "|[2.0]| [1.0]|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.MinMaxScaler(self, min=0.0, max=1.0, inputCol=None, outputCol=None)¶\n",
    "# 使用列汇总统计信息，将每个特征单独重新标定为一个常用范围[min，max]，这也称为最小 - 最大标准化或重新标定（注意由于零值可能会被转换为非零值，因此即使对于稀疏输入，转换器的输出也将是DenseVector）。 特征E的重新缩放的值被计算为，数据将被缩放到[0.0,1.0]范围内。 Rescaled(e_i) = (e_i - E_min) / (E_max - E_min) (max - min) + min For the case E_max == E_min, Rescaled(e_i) = 0.5 (max + min)\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "mmScaler = MinMaxScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = mmScaler.fit(df)\n",
    "print(model.originalMin, model.originalMax)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:09.862573Z",
     "start_time": "2019-06-05T12:51:09.601903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|    inputTokens|              nGrams|\n",
      "+---------------+--------------------+\n",
      "|[a, b, c, d, e]|[a b, b c, c d, d e]|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.NGram(n=2, inputCol=None, outputCol=None)¶\n",
    "# 一种功能转换器，用于将输入的字符串数组转换为n-gram数组。输入数组中的空值将被忽略。它返回一个n-gram数组，其中每个n-gram由一个以空格分隔的单词串表示。当输入为空时，返回一个空数组。当输入数组长度小于n（每n-gram的元素数）时，不返回n-gram。\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(inputTokens=[\"a\", \"b\", \"c\", \"d\", \"e\"])])\n",
    "ngram = NGram(n=2, inputCol=\"inputTokens\", outputCol=\"nGrams\")\n",
    "ngram.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:09.988059Z",
     "start_time": "2019-06-05T12:51:09.865252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    inputTokens|            nGrams|\n",
      "+---------------+------------------+\n",
      "|[a, b, c, d, e]|[a b c d, b c d e]|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 更改 n-gram 长度\n",
    "ngram.setParams(n=4).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:10.144501Z",
     "start_time": "2019-06-05T12:51:09.990607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    inputTokens|            output|\n",
      "+---------------+------------------+\n",
      "|[a, b, c, d, e]|[a b c d, b c d e]|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 临时修改输出列\n",
    "ngram.transform(df, {ngram.outputCol: \"output\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:10.350248Z",
     "start_time": "2019-06-05T12:51:10.147660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+\n",
      "|     dense|             sparse|  features|\n",
      "+----------+-------------------+----------+\n",
      "|[3.0,-4.0]|(4,[1,3],[4.0,3.0])|[0.6,-0.8]|\n",
      "+----------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.Normalizer(self, p=2.0, inputCol=None, outputCol=None)¶\n",
    "# 使用给定的p范数标准化矢量以得到单位范数（默认为L2）。\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "svec = Vectors.sparse(4, {1: 4.0, 3: 3.0})\n",
    "df = spark.createDataFrame([(Vectors.dense([3.0, -4.0]), svec)], [\"dense\", \"sparse\"])\n",
    "normalizer = Normalizer(p=2.0, inputCol=\"dense\", outputCol=\"features\")\n",
    "normalizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:10.518755Z",
     "start_time": "2019-06-05T12:51:10.353326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|     dense|             sparse|              freqs|\n",
      "+----------+-------------------+-------------------+\n",
      "|[3.0,-4.0]|(4,[1,3],[4.0,3.0])|(4,[1,3],[0.8,0.6])|\n",
      "+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer.setParams(inputCol=\"sparse\", outputCol=\"freqs\").transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:10.936912Z",
     "start_time": "2019-06-05T12:51:10.521273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|input|       output|\n",
      "+-----+-------------+\n",
      "|  0.0|(2,[0],[1.0])|\n",
      "|  1.0|(2,[1],[1.0])|\n",
      "|  2.0|    (2,[],[])|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.OneHotEncoderEstimator(inputCols=None, outputCols=None, handleInvalid='error', dropLast=True)¶\n",
    "# (分类列编码为二进制向量列) 一个热门的编码器，将一列类别索引映射到一列二进制向量，每行至多有一个单值，表示输入类别索引。 例如，对于5个类别，输入值2.0将映射到[0.0，0.0，1.0，0.0]的输出向量。 最后一个类别默认不包含（可通过dropLast进行配置），因为它使向量条目总和为1，因此线性相关。 所以一个4.0的输入值映射到[0.0，0.0，0.0，0.0]。这与scikit-learn的OneHotEncoder不同，后者保留所有类别。 输出向量是稀疏的。 当handleInvalid配置为“keep”时，会添加一个指示无效值的额外“类别”作为最后一个类别。因此，当dropLast为true时，无效值将被编码为全零向量。\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(0.0,), (1.0,), (2.0,)], [\"input\"])\n",
    "ohe = OneHotEncoderEstimator(inputCols=[\"input\"], outputCols=[\"output\"])\n",
    "model = ohe.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:11.950052Z",
     "start_time": "2019-06-05T12:51:10.946215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------------------+\n",
      "|features             |pca_features                            |\n",
      "+---------------------+----------------------------------------+\n",
      "|(5,[1,3],[1.0,7.0])  |[1.6485728230883807,-4.013282700516296] |\n",
      "|[2.0,0.0,3.0,4.0,5.0]|[-4.645104331781534,-1.1167972663619026]|\n",
      "|[4.0,0.0,0.0,6.0,7.0]|[-6.428880535676489,-5.337951427775355] |\n",
      "+---------------------+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([0.7944, 0.2056])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.ml.feature.PCA(self, k=None, inputCol=None, outputCol=None)¶\n",
    "# PCA训练一个模型将向量投影到前k个主成分的较低维空间。\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "     (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "     (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df)\n",
    "model.transform(df).show(truncate=0)\n",
    "\n",
    "model.explainedVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:12.433526Z",
     "start_time": "2019-06-05T12:51:11.952790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|values|buckets|\n",
      "+------+-------+\n",
      "|   0.1|    0.0|\n",
      "|   0.4|    1.0|\n",
      "|   1.2|    1.0|\n",
      "|   1.5|    1.0|\n",
      "|   NaN|    2.0|\n",
      "|   NaN|    2.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.QuantileDiscretizer(self, numBuckets=2, inputCol=None, outputCol=None, relativeError=0.001, handleInvalid=\"error\")¶\n",
    "# 与Bucketizer方法类似，但QuantileDiscretizer采用具有连续特征的列，并输出具有分箱分类特征的列。可以使用numBuckets参数设置区域的数量。所使用的桶的数量可能小于该值，例如，如果输入的不同值太少而不能创建足够的不同分位数。nan会占用一个新的分类\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, [\"values\"])\n",
    "qds = QuantileDiscretizer(numBuckets=2,\n",
    "     inputCol=\"values\", outputCol=\"buckets\", relativeError=0.01, handleInvalid=\"error\")\n",
    "bucketizer = qds.fit(df)\n",
    "qds.setHandleInvalid(\"keep\").fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:12.609759Z",
     "start_time": "2019-06-05T12:51:12.436817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  text|    words|\n",
      "+------+---------+\n",
      "|A B  c|[a, b, c]|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.RegexTokenizer(minTokenLength=1, gaps=True, pattern='\\s+', inputCol=None, outputCol=None, toLowercase=True)¶\n",
    "# 基于java正则表达式的标记生成器\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "df = spark.createDataFrame([(\"A B  c\",)], [\"text\"])\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "reTokenizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:14.228845Z",
     "start_time": "2019-06-05T12:51:12.612192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id| v1| v2|\n",
      "+---+---+---+\n",
      "|  0|1.0|3.0|\n",
      "|  2|2.0|5.0|\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.SQLTransformer(statement=None)¶\n",
    "# 实现由SQL语句定义的转换。目前我们只支持SQL语法，\n",
    "\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "df = spark.createDataFrame([(0, 1.0, 3.0), (2, 2.0, 5.0)], [\"id\", \"v1\", \"v2\"])\n",
    "sqlTrans = SQLTransformer(\n",
    "     statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "df.show()\n",
    "sqlTrans.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:14.565099Z",
     "start_time": "2019-06-05T12:51:14.232820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0] [1.4142135623730951]\n",
      "+-----+-------------------+\n",
      "|    a|             scaled|\n",
      "+-----+-------------------+\n",
      "|[0.0]|              [0.0]|\n",
      "|[2.0]|[1.414213562373095]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.StandardScaler(self, withMean=False, withStd=True, inputCol=None, outputCol=None)¶\n",
    "# (标准化列，使其拥有零均值和等于1的标准差) 通过使用训练集中样本的列汇总统计消除平均值和缩放到单位方差来标准化特征。使用校正后的样本标准偏差计算“单位标准差”，该标准偏差计算为无偏样本方差的平方根。\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "standardScaler = StandardScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = standardScaler.fit(df)\n",
    "print(model.mean, model.std)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:15.088984Z",
     "start_time": "2019-06-05T12:51:14.568552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     text| words|\n",
      "+---------+------+\n",
      "|[a, b, c]|[a, c]|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.StopWordsRemover(inputCol=None, outputCol=None, stopWords=None, caseSensitive=False)¶\n",
    "# 一个特征转换器，用于过滤掉输入中的停用词。\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n",
    "remover = StopWordsRemover(inputCol=\"text\", outputCol=\"words\", stopWords=[\"b\"])\n",
    "remover.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:15.275180Z",
     "start_time": "2019-06-05T12:51:15.093614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    text|       words|\n",
      "+--------+------------+\n",
      "|ASD VA c|[asd, va, c]|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.Tokenizer(inputCol=None, outputCol=None)¶\n",
    "# 一个标记生成器，它将输入字符串转换为小写，然后用空格分隔它。\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "df = spark.createDataFrame([(\"ASD VA c\",)], [\"text\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:15.523924Z",
     "start_time": "2019-06-05T12:51:15.277742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+\n",
      "|features               |sliced    |\n",
      "+-----------------------+----------+\n",
      "|[-2.0,2.3,0.0,0.0,1.0] |[2.3,1.0] |\n",
      "|[0.0,0.0,0.0,0.0,0.0]  |[0.0,0.0] |\n",
      "|[0.6,-1.1,-3.0,4.5,3.3]|[-1.1,3.3]|\n",
      "+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.VectorSlicer(inputCol=None, outputCol=None, indices=None, names=None)¶\n",
    "# 此类采用特征向量并输出具有原始特征的子阵列的新特征向量。 可以使用索引（setIndices（））或名称（setNames（））指定要素子集。必须至少选择一个功能。不允许使用重复的功能，因此所选索引和名称之间不能重叠。 输出向量将首先按所选索引（按给定顺序）排序要素，然后是所选名称（按给定顺序）。\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([\n",
    "     (Vectors.dense([-2.0, 2.3, 0.0, 0.0, 1.0]),),\n",
    "     (Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0]),),\n",
    "     (Vectors.dense([0.6, -1.1, -3.0, 4.5, 3.3]),)], [\"features\"])\n",
    "vs = VectorSlicer(inputCol=\"features\", outputCol=\"sliced\", indices=[1, 4])\n",
    "vs.transform(df).show(truncate=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:15.746372Z",
     "start_time": "2019-06-05T12:51:15.539164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------+\n",
      "|  a|  b|  c|     features|\n",
      "+---+---+---+-------------+\n",
      "|  1|  0|  3|[1.0,0.0,3.0]|\n",
      "+---+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.VectorAssembler(inputCols=None, outputCol=None)¶\n",
    "# 将多个列合并到向量列中的要素转换器。\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
    "vecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n",
    "vecAssembler.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:16.597640Z",
     "start_time": "2019-06-05T12:51:15.752910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|word|              vector|\n",
      "+----+--------------------+\n",
      "|   a|[0.09461779892444...|\n",
      "|   b|[1.15474212169647...|\n",
      "|   c|[-0.3794820010662...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)¶\n",
    "# Word2Vec训练Map（String，Vector）模型，即将单词转换为代码以进行进一步的自然语言处理或机器学习过程。\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "sent = (\"a b \" * 100 + \"a c \" * 10).split(\" \")\n",
    "doc = spark.createDataFrame([(sent,), (sent,)], [\"sentence\"])\n",
    "word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"sentence\", outputCol=\"model\")\n",
    "model = word2Vec.fit(doc)\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:16.685153Z",
     "start_time": "2019-06-05T12:51:16.602115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|word|         similarity|\n",
      "+----+-------------------+\n",
      "|   b|0.25053444504737854|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('b', 0.25053444504737854)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找相似字符\n",
    "model.findSynonyms(\"a\", 1).show()\n",
    "model.findSynonymsArray(\"a\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:16.785975Z",
     "start_time": "2019-06-05T12:51:16.689321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|word|similarity|\n",
      "+----+----------+\n",
      "|   b|     0.251|\n",
      "|   c|    -0.698|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number as fmt\n",
    "model.findSynonyms(\"a\", 2).select(\"word\", fmt(\"similarity\", 3).alias(\"similarity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml.regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:17.075651Z",
     "start_time": "2019-06-05T12:51:16.789351Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO| BLACK|LSTAT|MEDV|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296.0|   15.3| 396.9| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242.0|   17.8| 396.9| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242.0|   17.8|392.83| 4.03|34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222.0|   18.7|394.63| 2.94|33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222.0|   18.7| 396.9| 5.33|36.2|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222.0|   18.7|394.12| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311.0|   15.2| 395.6|12.43|22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311.0|   15.2| 396.9|19.15|27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311.0|   15.2|386.63|29.93|16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311.0|   15.2|386.71| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311.0|   15.2|392.52|20.45|15.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311.0|   15.2| 396.9|13.27|18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311.0|   15.2| 390.5|15.71|21.7|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307.0|   21.0| 396.9| 8.26|20.4|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307.0|   21.0|380.02|10.26|18.2|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307.0|   21.0|395.62| 8.47|19.9|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307.0|   21.0|386.85| 6.58|23.1|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307.0|   21.0|386.75|14.67|17.5|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307.0|   21.0|288.99|11.69|20.2|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307.0|   21.0|390.95|11.28|18.2|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CRIM--  城镇人均犯罪率。\n",
    "# ZN  - 占地面积超过25,000平方英尺的住宅用地比例。\n",
    "# INDUS  - 每个城镇非零售业务的比例。\n",
    "# CHAS  - Charles River虚拟变量（如果河流经过则= 1;否则为0）。\n",
    "# NOX  - 氮氧化物浓度（每千万份）。\n",
    "# RM  - 每间住宅的平均房间数。\n",
    "# AGE  - 1940年以前建造的自住单位比例。\n",
    "# DIS  - 加权平均值到五个波士顿就业中心的距离。\n",
    "# RAD  - 径向高速公路的可达性指数。\n",
    "# TAX  - 每10,000美元的全额物业税率。\n",
    "# PTRATIO  - 城镇的学生与教师比例。\n",
    "# BLACK  - 1000（Bk - 0.63）²其中Bk是城镇黑人的比例。\n",
    "# LSTAT  - 人口较低的地位（百分比）。\n",
    "# MEDV  - 自住房屋的中位数价值1000美元。这是目标变量。\n",
    "import pandas as pd\n",
    "df0 = pd.read_csv('housing.data',header=None,delim_whitespace=True)\n",
    "columns=\"CRIM:float,ZN:float,INDUS:float,CHAS:int,NOX:float,RM:float,AGE:float,DIS:float,\\\n",
    "         RAD:int,TAX:float,PTRATIO:float,BLACK:float,LSTAT:float,MEDV:float\"\n",
    "#df0.head()\n",
    "df=spark.createDataFrame(df0,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:17.153642Z",
     "start_time": "2019-06-05T12:51:17.078639Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "def feature_converter(df):\n",
    "    vecAss = VectorAssembler(inputCols=df.columns[0:-1], outputCol='features')\n",
    "    df_va = vecAss.transform(df)\n",
    "    return df_va\n",
    "\n",
    "train_data, test_data = feature_converter(df).select(['features', 'medv']).randomSplit([7.0, 3.0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:17.552824Z",
     "start_time": "2019-06-05T12:51:17.156588Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 150)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count(),test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:35:26.525334Z",
     "start_time": "2019-06-05T12:35:23.107497Z"
    }
   },
   "source": [
    "### 决策树回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:18.300072Z",
     "start_time": "2019-06-05T12:51:17.555945Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark.ml.regression.DecisionTreeRegressor(featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='variance', seed=None, varianceCol=None)\n",
    "# fit(dataset, params=None)方法 \n",
    "# Impurity: 信息增益计算准则，支持选项：variance \n",
    "# maxBins: 连续特征离散化的最大分箱个数， >=2并且>=任何分类特征的分类个数 \n",
    "# maxDepth: 最大树深 \n",
    "# minInfoGain: 分割节点所需最小信息增益 \n",
    "# minInstancesPerNode: 分割后每个子节点最小实例个数\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(maxDepth=5, varianceCol=\"variance\", labelCol='medv')\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:18.331317Z",
     "start_time": "2019-06-05T12:51:18.306675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(13, {0: 0.0637, 2: 0.013, 4: 0.0039, 5: 0.6454, 6: 0.0014, 7: 0.0466, 9: 0.0111, 10: 0.0115, 11: 0.0033, 12: 0.1999})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:18.558217Z",
     "start_time": "2019-06-05T12:51:18.337596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+-----------------+\n",
      "|            features|medv|        prediction|         variance|\n",
      "+--------------------+----+------------------+-----------------+\n",
      "|[0.00906000006943...|32.2|32.839999771118165|4.432399835205979|\n",
      "|[0.01095999963581...|22.0|20.900980444515454|7.065194933157281|\n",
      "|[0.01439000014215...|29.1|31.089999961853028| 3.97289825973603|\n",
      "+--------------------+----+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dt_model.transform(test_data)\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:18.722934Z",
     "start_time": "2019-06-05T12:51:18.561537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据的均方根误差（rmse）:4.296898321932952\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "dt_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"rmse\", predictionCol='prediction')\n",
    "rmse = dt_evaluator.evaluate(result)\n",
    "print('测试数据的均方根误差（rmse）:{}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 梯度提升树回归 （Gradient-boosted tree regression）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:22.098957Z",
     "start_time": "2019-06-05T12:51:18.726040Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pyspark.ml.regression.GBTRegressor(featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, subsamplingRate=1.0, checkpointInterval=10, lossType='squared', maxIter=20, stepSize=0.1, seed=None, impurity='variance')\n",
    "# fit(dataset,params=None)方法 \n",
    "# lossType: GBT要最小化的损失函数，可选：squared, absolute  \n",
    "# maxIter: 最大迭代次数 \n",
    "# stepSize: 每次优化迭代的步长 \n",
    "# subsamplingRate:用于训练每颗决策树的训练数据集的比例，区间[0,1]\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(maxIter=10, labelCol='medv', maxDepth=3)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:22.116947Z",
     "start_time": "2019-06-05T12:51:22.101851Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(13, {0: 0.0393, 1: 0.007, 2: 0.0105, 4: 0.0272, 5: 0.2798, 6: 0.0546, 7: 0.0208, 8: 0.0318, 9: 0.1025, 10: 0.107, 12: 0.3194})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:22.324772Z",
     "start_time": "2019-06-05T12:51:22.119927Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|            features|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[0.00906000006943...|32.2| 33.11554937488464|\n",
      "|[0.01095999963581...|22.0| 22.78796611388136|\n",
      "|[0.01439000014215...|29.1|27.976121828017796|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = gbt_model.transform(test_data)\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:22.345886Z",
     "start_time": "2019-06-05T12:51:22.331812Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.treeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:22.481352Z",
     "start_time": "2019-06-05T12:51:22.348801Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据的均方根误差（rmse）:4.395048095005025\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "gbt_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"rmse\", predictionCol='prediction')\n",
    "rmse = gbt_evaluator.evaluate(result)\n",
    "print('测试数据的均方根误差（rmse）:{}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归（LinearRegression）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:23.315252Z",
     "start_time": "2019-06-05T12:51:22.484329Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark.ml.regression.LinearRegression(featuresCol='features', labelCol='label', predictionCol='prediction', maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-06, fitIntercept=True, standardization=True, solver='auto', weightCol=None, aggregationDepth=2, loss='squaredError', epsilon=1.35)\n",
    "# 学习目标是通过正规化最小化指定的损失函数。这支持两种损失：\n",
    "# squaredError (a.k.a 平方损失)\n",
    "# huber (对于相对较小的误差和相对大的误差的绝对误差的平方误差的混合，我们从训练数据估计比例参数)\n",
    "# 支持多种类型的正则化：\n",
    "# None：OLS\n",
    "# L2：ridge回归\n",
    "# L1：Lasso回归\n",
    "# L1+L2：elastic回归\n",
    "# 注意：与huber loss匹配仅支持none和L2正规化。\n",
    "# aggregationDepth: 树聚合的深度, >=2 \n",
    "# elasticNtParam: ElasticNet混合参数，在[0,1]范围内，alpha=0为L2， alpha=1为L1 \n",
    "# fit(dataset,params=None)方法 \n",
    "# fitIntercept: 是否拟合截距 \n",
    "# maxIter: 最大迭代次数 \n",
    "# regParam：正则化参数 >=0 \n",
    "# solver: 优化算法，没设置或空则使用”auto” \n",
    "# standardization: 是否对拟合模型的特征进行标准化\n",
    "# Summary属性\n",
    "# coefficientStandardErrors \n",
    "# devianceResiduals: 加权残差 \n",
    "# explainedVariance: 返回解释的方差回归得分，explainedVariance=1−variance(y−(̂ y))/variance(y) \n",
    "# meanAbsoluteError: 返回均值绝对误差 \n",
    "# meanSquaredError: 返回均值平方误 \n",
    "# numInstances: 预测的实例个数 \n",
    "# pValues: 系数和截距的双边P值，只有用”normal”solver才可用 \n",
    "# predictions: 模型transform方法返回的预测 \n",
    "# r2: R方 \n",
    "# residuals: 残差 \n",
    "# rootMeanSquaredError: 均方误差平方根 \n",
    "# tValues： T统计量\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, elasticNetParam=0.8, regParam=0.3, labelCol='medv')\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:23.327458Z",
     "start_time": "2019-06-05T12:51:23.318168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.485821\n",
      "r2: 0.768820\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:23.477395Z",
     "start_time": "2019-06-05T12:51:23.333829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|            features|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[0.00906000006943...|32.2|32.541632830568545|\n",
      "|[0.01095999963581...|22.0| 28.50151198559133|\n",
      "|[0.01439000014215...|29.1| 30.73873468167102|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = lr_model.transform(test_data)\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T12:51:23.628021Z",
     "start_time": "2019-06-05T12:51:23.480112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R平方（r2）:0.493\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"r2\", predictionCol='prediction')\n",
    "r2 = lr_evaluator.evaluate(result)\n",
    "print('R平方（r2）:{:.3}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T16:15:53.082433Z",
     "start_time": "2019-06-05T16:15:51.792940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:6.27\n",
      "r2:0.493\n"
     ]
    }
   ],
   "source": [
    "test_evaluation = lr_model.evaluate(test_data)\n",
    "print('RMSE:{:.3}'.format(test_evaluation.rootMeanSquaredError))\n",
    "print('r2:{:.3}'.format(test_evaluation.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "144px",
    "left": "654px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
